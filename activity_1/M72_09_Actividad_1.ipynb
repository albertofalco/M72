{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/albertofalco/M7209/blob/main/M72_09_Actividad_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "ME72: Maestría en Métodos Cuantitativos para la Gestión y Análisis de Datos\n",
        "M72109: Gestión de datos no estructurados\n",
        "Universidad de Buenos Aires - Facultad de Ciencias Economicas (UBA-FCE)\n",
        "Año: 2023\n",
        "\n",
        "Profesor: Facundo Santiago\n",
        "\n",
        "Alumno: Alberto Falco\n",
        "```"
      ],
      "metadata": {
        "id": "ltpW0SBLPRyL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "it108cAwxRJt"
      },
      "source": [
        "# Modelado Clásico - Actividad 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VYPdzDTxRJw"
      },
      "source": [
        "Topic modeling es una técnica de aprendizaje automático no supervisado donde intentados descubrir tópicos que son abstractos al texto pero que pueden describir una colección de documentos. Es importante marcar que estos \"tópicos\" no son necesariamente equivalentes a la interpretación coloquial de tópicos, sino que responden a un patrón que emerge de las palabras que están en los documentos.\n",
        "\n",
        "La suposición básica para Topic Modeling es que cada documento está representado por una mescla de tópicos, y cada tópico consite en una conlección de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VomDdG0PxRJx"
      },
      "source": [
        "## 1. Direcciones\n",
        "Intentaremos construir un pipeline de machine learning donde como entrada recibamos texto, ejecutemos todos los pasos que vimos en este notebook incluyendo:\n",
        "\n",
        " - Eliminación de stopwords\n",
        " - Tokenización\n",
        " - Stemming y Lemmatization\n",
        " - Procesamiento especico del tema\n",
        " - Creación de features utilizando algun metodo de reducción de dimensionalidad, SVD, LSI, LDA\n",
        "\n",
        ", para luego utilizar estas features para entrenar un modelo que nos permita predecir alguna propiedad interesante del set de datos. En este caso en particular, donde estermos analizando tweets, predeciremos el sector al que pertenece el tweet: Alimentación, Bebidas, etc.\n",
        "\n",
        "En esta actividad les proponemos realizar cambios en alguna de las etapas del procesamiento para modificar la performance del modelo resultante y evaluar que cambios generan el mejor modelo resultante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrD82rvfxRJy"
      },
      "source": [
        "<img src='https://github.com/santiagxf/M72109/blob/master/docs/nlp/_images/classic_pipeline.png?raw=1' />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iw-sykcZWAb"
      },
      "source": [
        "### 1.1. Para ejecutar este notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ke-vHIJZWAc"
      },
      "source": [
        "Para ejecutar este notebook, instale las siguientes librerias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21Xgn3NqxRJ7"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/santiagxf/M72109/master/NLP/Datasets/mascorpus/tweets_marketing.csv --quiet --no-clobber --directory-prefix ./Datasets/mascorpus/\n",
        "! wget https://raw.githubusercontent.com/albertofalco/test/main/topic-modeling.txt --quiet --no-clobber"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zUkv0yIuHM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd5f6ade-77a3-45b6-e7db-74b32a857ae6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -r topic-modeling.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOGkFv6YZWAg",
        "outputId": "3a10dd66-318f-425e-9fb5-41deadfbb1fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-26 02:42:21.770934: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-26 02:42:21.771004: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-26 02:42:21.771062: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-26 02:42:21.784272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-26 02:42:23.937407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "! python -m spacy download es_core_news_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUi2gJQksrn2"
      },
      "outputs": [],
      "source": [
        "! wget https://raw.githubusercontent.com/santiagxf/M72109/master/m72109/nlp/normalization.py --quiet --no-clobber --directory-prefix ./m72109/nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPpqVNrwSdhL"
      },
      "source": [
        "Primero importaremos algunas librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPfF_O0U_J9a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE_O7bEjLebd"
      },
      "source": [
        "### 1.2. Sobre el set de datos con el que vamos a trabajar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8lcRTa_Li4e"
      },
      "source": [
        "Utilizaremos como ejemplo un set de datos en español que contiene tweets que diferentes usuarios han publicado en relación a diferentes marcas de productos u empresas en el rubro de alimentación, construcción, automoviles, etc. Estos tweets, a su vez, están asociados a una de las diferentes fases en el proceso de ventas (también conocido como Marketing Funel) y por eso están tagueados con las fases de:\n",
        " - Awareness – el cliente es conciente de la existencia de un producto o servicio\n",
        " - Interest – activamente expresa el interes de un producto o servicio\n",
        " - Evaluation – aspira una marca o producto en particular\n",
        " - Purchase – toma el siguiente paso necesario para comprar el producto o servicio\n",
        " - Postpurchase - realización del proceso de compra. El cliente compara la diferencia entre lo que deseaba y lo que obtuvo\n",
        "\n",
        "Referencia: [Spanish Corpus of Tweets for Marketing](http://ceur-ws.org/Vol-2111/paper1.pdf\n",
        "\n",
        "> Nota: La version de este conjunto de datos que utilizaremos aqui es una versión preprocesada del original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnFmjMFkxRJ4"
      },
      "outputs": [],
      "source": [
        "tweets = pd.read_csv('Datasets/mascorpus/tweets_marketing.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYwp3XEcZWAl"
      },
      "source": [
        "## 2. Desarrollo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se8eN3hDZWAl"
      },
      "source": [
        "### 2.1. Creando nuestros sets de datos de entrenamiento y testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ6wpf2wxRKT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'],\n",
        "                                                    test_size=0.33,\n",
        "                                                    stratify=tweets['SECTOR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n_LptW1xRJ-"
      },
      "source": [
        "### 2.2. Construcción del modelo: Pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm4omFKExRJ_"
      },
      "source": [
        "**Paso 1:** Instanciamos nuestro preprocesamiento de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbt-OdJFxRKC"
      },
      "outputs": [],
      "source": [
        "from m72109.nlp.normalization import TweetTextNormalizer\n",
        "\n",
        "normalizer = TweetTextNormalizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwFAaJpjsrn5"
      },
      "source": [
        "> Tip: Inspeccione todos los parametros de `TweetTextNormalizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFY6eUe4xRKF"
      },
      "source": [
        "**Paso 2:** Instanciamos nuestro vectorizador, en este caso usando el método TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oMARrKNxRKG"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atOW2STAxRKI"
      },
      "source": [
        "**Paso 3:** Instanciamos nuestro generador de features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUvY-6AjxRKJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "featurizer = LatentDirichletAllocation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdi6dwy_xRKL"
      },
      "source": [
        "**Paso 4:** Instanciamos nuestro clasificador que utilizará las features generadas hasta este momento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wspdz2OZxRKL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "estimator = LogisticRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBLDj74FZWAq"
      },
      "source": [
        "### 2.3. Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk5oDgUpxRKN"
      },
      "source": [
        "Ensamblamos el pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVA3dR60xRKO"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline(steps=[('normalizer', normalizer),\n",
        "                          ('vectorizer', vectorizer),\n",
        "                          ('featurizer', featurizer),\n",
        "                          ('estimator', estimator)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCHelkwGxRKS"
      },
      "source": [
        "### 2.4. Evaluación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXQsgKLaxRKT"
      },
      "source": [
        "**Evaluación:** Entrenamos el modelo y testeamos su performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeSBrVY3xRKW",
        "outputId": "1fea288d-c26f-47e6-fac8-cb673f80f374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2521/2521 [03:15<00:00, 12.90it/s]\n"
          ]
        }
      ],
      "source": [
        "model = pipeline.fit(X=X_train, y=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbZSy77rxRKY",
        "outputId": "5fd22b63-90e9-4a8e-89c4-82414a686a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1242/1242 [01:29<00:00, 13.93it/s]\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ94y_xpxRKa",
        "outputId": "180f9cf6-0532-449a-8826-a27c36208127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.41      0.09      0.15       148\n",
            "       BANCA       0.33      0.10      0.15       198\n",
            "     BEBIDAS       0.30      0.24      0.27       223\n",
            "    DEPORTES       0.28      0.33      0.30       216\n",
            "      RETAIL       0.25      0.66      0.36       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.27      1242\n",
            "   macro avg       0.22      0.20      0.18      1242\n",
            "weighted avg       0.26      0.27      0.22      1242\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiiWoSQOLj3g"
      },
      "source": [
        "## 3. Comparación automática de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpPAuwqzNRrY",
        "outputId": "1d26446a-ca92-4fd8-8de2-44d24a5232d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2521/2521 [02:38<00:00, 15.86it/s]\n",
            "100%|██████████| 1242/1242 [01:13<00:00, 16.81it/s]\n"
          ]
        }
      ],
      "source": [
        "# Importacion de librerias\n",
        "from sklearn.model_selection import train_test_split\n",
        "from m72109.nlp.normalization import TweetTextNormalizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "import itertools\n",
        "import warnings\n",
        "\n",
        "# Desactivacion de warnings.\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Split del dataset.\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweets['TEXTO'], tweets['SECTOR'],\n",
        "                                                    test_size=0.33,\n",
        "                                                    stratify=tweets['SECTOR'])\n",
        "\n",
        "# Normalización.\n",
        "normalizer = TweetTextNormalizer(language=\"spanish\")\n",
        "X_train_normalized = normalizer.fit_transform(X_train, y_train)\n",
        "X_test_normalized = normalizer.fit_transform(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Diseño del espacio de busqueda.\n",
        "search_space = {\"vectorizer\": [TfidfVectorizer(use_idf=True, sublinear_tf=True, norm='l2')],\n",
        "                \"featurizer\": [LatentDirichletAllocation(n_jobs=-1),\n",
        "                               LatentDirichletAllocation(n_jobs=-1, n_components=50),\n",
        "                               LatentDirichletAllocation(n_jobs=-1, n_components=100),\n",
        "                               LatentDirichletAllocation(n_jobs=-1, n_components=200),\n",
        "                               TruncatedSVD(),\n",
        "                               TruncatedSVD(n_components=50),\n",
        "                               TruncatedSVD(n_components=100),\n",
        "                               TruncatedSVD(n_components=200)\n",
        "                               ],\n",
        "                \"estimator\": [LogisticRegression(n_jobs=-1), GradientBoostingClassifier()]\n",
        "                }\n",
        "keys = list(search_space.keys())\n",
        "values = list(search_space.values())\n",
        "combinations = list(itertools.product(*values))\n",
        "dict_list = [dict(zip(keys, combination)) for combination in combinations]\n",
        "\n",
        "# Función para entrenamiento y predicción.\n",
        "def train_predict(X_train, y_train, X_test, y_test, **kwargs):\n",
        "  pipeline = Pipeline(steps=[('vectorizer', kwargs[\"vectorizer\"]),\n",
        "                             ('featurizer', kwargs[\"featurizer\"]),\n",
        "                             ('estimator', kwargs[\"estimator\"])])\n",
        "  model = pipeline.fit(X=X_train, y=y_train)\n",
        "  predictions = model.predict(X_test)\n",
        "  report = classification_report(y_test, predictions)\n",
        "  return model, predictions, report\n",
        "\n",
        "# Iteración sobre espacio de búsqueda.\n",
        "results = []\n",
        "for _, comb in enumerate(dict_list):\n",
        "  result = {}\n",
        "  result['Combination'] = comb\n",
        "  print(\"Fitting model number {}: {}...\".format(_, str(comb)))\n",
        "  result['Info'] = train_predict(X_train_normalized, y_train, X_test_normalized, y_test, **comb)\n",
        "  results.append(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXdWBUZeGEmQ",
        "outputId": "d4ee3e60-840c-4186-fa4d-2f1db1f5b582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting model number 0: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 1: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_jobs=-1), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 2: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=50, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 3: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=50, n_jobs=-1), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 4: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=100, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 5: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=100, n_jobs=-1), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 6: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=200, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 7: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=200, n_jobs=-1), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 8: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 9: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 10: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=50), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 11: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=50), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 12: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=100), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 13: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=100), 'estimator': GradientBoostingClassifier()}...\n",
            "Fitting model number 14: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=200), 'estimator': LogisticRegression(n_jobs=-1)}...\n",
            "Fitting model number 15: {'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=200), 'estimator': GradientBoostingClassifier()}...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtención de resultados.\n",
        "for result in results:\n",
        "  print(result['Combination'], end = '\\n')\n",
        "  print(result['Info'][2])\n",
        "  print('Number of components: {}'.format(result['Info'][0][1].n_components), end = '\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Qh-kwO0N2Aj",
        "outputId": "baff1f7b-a079-496e-82c4-2137de9dffd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.00      0.00      0.00       148\n",
            "       BANCA       0.40      0.14      0.20       198\n",
            "     BEBIDAS       0.31      0.21      0.25       223\n",
            "    DEPORTES       0.31      0.37      0.34       216\n",
            "      RETAIL       0.27      0.76      0.39       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.29      1242\n",
            "   macro avg       0.18      0.21      0.17      1242\n",
            "weighted avg       0.23      0.29      0.22      1242\n",
            "\n",
            "Number of components: 10\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_jobs=-1), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.15      0.35      0.21       110\n",
            "  AUTOMOCION       0.29      0.20      0.23       148\n",
            "       BANCA       0.52      0.09      0.15       198\n",
            "     BEBIDAS       0.31      0.42      0.36       223\n",
            "    DEPORTES       0.31      0.35      0.33       216\n",
            "      RETAIL       0.30      0.32      0.31       268\n",
            "       TELCO       0.29      0.06      0.10        79\n",
            "\n",
            "    accuracy                           0.28      1242\n",
            "   macro avg       0.31      0.26      0.24      1242\n",
            "weighted avg       0.32      0.28      0.26      1242\n",
            "\n",
            "Number of components: 10\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=50, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.38      0.07      0.11       148\n",
            "       BANCA       0.32      0.18      0.23       198\n",
            "     BEBIDAS       0.35      0.36      0.36       223\n",
            "    DEPORTES       0.41      0.45      0.43       216\n",
            "      RETAIL       0.27      0.65      0.39       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.32      1242\n",
            "   macro avg       0.25      0.24      0.22      1242\n",
            "weighted avg       0.29      0.32      0.27      1242\n",
            "\n",
            "Number of components: 50\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=50, n_jobs=-1), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.22      0.26      0.24       110\n",
            "  AUTOMOCION       0.34      0.21      0.26       148\n",
            "       BANCA       0.40      0.15      0.21       198\n",
            "     BEBIDAS       0.33      0.54      0.41       223\n",
            "    DEPORTES       0.38      0.37      0.37       216\n",
            "      RETAIL       0.30      0.42      0.35       268\n",
            "       TELCO       1.00      0.05      0.10        79\n",
            "\n",
            "    accuracy                           0.33      1242\n",
            "   macro avg       0.42      0.28      0.28      1242\n",
            "weighted avg       0.38      0.33      0.31      1242\n",
            "\n",
            "Number of components: 50\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=100, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.58      0.19      0.29       110\n",
            "  AUTOMOCION       0.53      0.18      0.27       148\n",
            "       BANCA       0.42      0.26      0.32       198\n",
            "     BEBIDAS       0.44      0.46      0.45       223\n",
            "    DEPORTES       0.49      0.56      0.52       216\n",
            "      RETAIL       0.31      0.64      0.42       268\n",
            "       TELCO       1.00      0.03      0.05        79\n",
            "\n",
            "    accuracy                           0.40      1242\n",
            "   macro avg       0.54      0.33      0.33      1242\n",
            "weighted avg       0.48      0.40      0.38      1242\n",
            "\n",
            "Number of components: 100\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=100, n_jobs=-1), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.50      0.46      0.48       110\n",
            "  AUTOMOCION       0.39      0.21      0.27       148\n",
            "       BANCA       0.54      0.38      0.45       198\n",
            "     BEBIDAS       0.43      0.62      0.51       223\n",
            "    DEPORTES       0.52      0.58      0.55       216\n",
            "      RETAIL       0.38      0.50      0.43       268\n",
            "       TELCO       1.00      0.04      0.07        79\n",
            "\n",
            "    accuracy                           0.45      1242\n",
            "   macro avg       0.54      0.40      0.39      1242\n",
            "weighted avg       0.49      0.45      0.43      1242\n",
            "\n",
            "Number of components: 100\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=200, n_jobs=-1), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.74      0.34      0.46       110\n",
            "  AUTOMOCION       0.66      0.53      0.59       148\n",
            "       BANCA       0.58      0.62      0.60       198\n",
            "     BEBIDAS       0.67      0.68      0.67       223\n",
            "    DEPORTES       0.92      0.81      0.86       216\n",
            "      RETAIL       0.49      0.76      0.60       268\n",
            "       TELCO       0.50      0.16      0.25        79\n",
            "\n",
            "    accuracy                           0.63      1242\n",
            "   macro avg       0.65      0.56      0.58      1242\n",
            "weighted avg       0.65      0.63      0.62      1242\n",
            "\n",
            "Number of components: 200\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': LatentDirichletAllocation(n_components=200, n_jobs=-1), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.78      0.67      0.72       110\n",
            "  AUTOMOCION       0.86      0.77      0.81       148\n",
            "       BANCA       0.81      0.79      0.80       198\n",
            "     BEBIDAS       0.77      0.85      0.81       223\n",
            "    DEPORTES       0.82      0.79      0.80       216\n",
            "      RETAIL       0.74      0.83      0.78       268\n",
            "       TELCO       0.82      0.75      0.78        79\n",
            "\n",
            "    accuracy                           0.79      1242\n",
            "   macro avg       0.80      0.78      0.79      1242\n",
            "weighted avg       0.80      0.79      0.79      1242\n",
            "\n",
            "Number of components: 200\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.00      0.00      0.00       110\n",
            "  AUTOMOCION       0.00      0.00      0.00       148\n",
            "       BANCA       0.20      0.01      0.02       198\n",
            "     BEBIDAS       0.46      0.59      0.52       223\n",
            "    DEPORTES       0.99      0.78      0.87       216\n",
            "      RETAIL       0.25      0.72      0.37       268\n",
            "       TELCO       0.00      0.00      0.00        79\n",
            "\n",
            "    accuracy                           0.40      1242\n",
            "   macro avg       0.27      0.30      0.25      1242\n",
            "weighted avg       0.34      0.40      0.33      1242\n",
            "\n",
            "Number of components: 2\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.19      0.05      0.09       110\n",
            "  AUTOMOCION       0.33      0.30      0.31       148\n",
            "       BANCA       0.31      0.31      0.31       198\n",
            "     BEBIDAS       0.69      0.57      0.62       223\n",
            "    DEPORTES       0.94      0.88      0.91       216\n",
            "      RETAIL       0.33      0.57      0.42       268\n",
            "       TELCO       0.25      0.08      0.12        79\n",
            "\n",
            "    accuracy                           0.47      1242\n",
            "   macro avg       0.43      0.39      0.40      1242\n",
            "weighted avg       0.48      0.47      0.46      1242\n",
            "\n",
            "Number of components: 2\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=50), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.99      0.83      0.90       110\n",
            "  AUTOMOCION       0.91      0.88      0.89       148\n",
            "       BANCA       0.86      0.90      0.88       198\n",
            "     BEBIDAS       0.99      0.89      0.94       223\n",
            "    DEPORTES       0.99      0.93      0.96       216\n",
            "      RETAIL       0.76      0.93      0.84       268\n",
            "       TELCO       0.97      0.81      0.88        79\n",
            "\n",
            "    accuracy                           0.90      1242\n",
            "   macro avg       0.92      0.88      0.90      1242\n",
            "weighted avg       0.91      0.90      0.90      1242\n",
            "\n",
            "Number of components: 50\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=50), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       1.00      0.85      0.92       110\n",
            "  AUTOMOCION       0.94      0.86      0.90       148\n",
            "       BANCA       0.89      0.91      0.90       198\n",
            "     BEBIDAS       0.93      0.92      0.93       223\n",
            "    DEPORTES       0.98      0.91      0.94       216\n",
            "      RETAIL       0.82      0.95      0.88       268\n",
            "       TELCO       0.91      0.89      0.90        79\n",
            "\n",
            "    accuracy                           0.91      1242\n",
            "   macro avg       0.92      0.90      0.91      1242\n",
            "weighted avg       0.91      0.91      0.91      1242\n",
            "\n",
            "Number of components: 50\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=100), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.99      0.89      0.94       110\n",
            "  AUTOMOCION       0.98      0.95      0.97       148\n",
            "       BANCA       0.90      0.91      0.90       198\n",
            "     BEBIDAS       0.97      0.90      0.93       223\n",
            "    DEPORTES       0.98      0.94      0.96       216\n",
            "      RETAIL       0.80      0.94      0.87       268\n",
            "       TELCO       0.97      0.84      0.90        79\n",
            "\n",
            "    accuracy                           0.92      1242\n",
            "   macro avg       0.94      0.91      0.92      1242\n",
            "weighted avg       0.93      0.92      0.92      1242\n",
            "\n",
            "Number of components: 100\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=100), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.98      0.90      0.94       110\n",
            "  AUTOMOCION       0.96      0.91      0.93       148\n",
            "       BANCA       0.87      0.91      0.89       198\n",
            "     BEBIDAS       0.93      0.92      0.93       223\n",
            "    DEPORTES       0.98      0.91      0.94       216\n",
            "      RETAIL       0.85      0.94      0.90       268\n",
            "       TELCO       0.91      0.90      0.90        79\n",
            "\n",
            "    accuracy                           0.92      1242\n",
            "   macro avg       0.93      0.91      0.92      1242\n",
            "weighted avg       0.92      0.92      0.92      1242\n",
            "\n",
            "Number of components: 100\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=200), 'estimator': LogisticRegression(n_jobs=-1)}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       1.00      0.93      0.96       110\n",
            "  AUTOMOCION       0.99      0.96      0.97       148\n",
            "       BANCA       0.92      0.94      0.93       198\n",
            "     BEBIDAS       0.96      0.91      0.94       223\n",
            "    DEPORTES       0.98      0.94      0.96       216\n",
            "      RETAIL       0.84      0.95      0.89       268\n",
            "       TELCO       0.97      0.90      0.93        79\n",
            "\n",
            "    accuracy                           0.94      1242\n",
            "   macro avg       0.95      0.93      0.94      1242\n",
            "weighted avg       0.94      0.94      0.94      1242\n",
            "\n",
            "Number of components: 200\n",
            "\n",
            "{'vectorizer': TfidfVectorizer(sublinear_tf=True), 'featurizer': TruncatedSVD(n_components=200), 'estimator': GradientBoostingClassifier()}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "ALIMENTACION       0.99      0.91      0.95       110\n",
            "  AUTOMOCION       0.96      0.90      0.93       148\n",
            "       BANCA       0.89      0.92      0.91       198\n",
            "     BEBIDAS       0.91      0.93      0.92       223\n",
            "    DEPORTES       0.98      0.89      0.93       216\n",
            "      RETAIL       0.85      0.95      0.89       268\n",
            "       TELCO       0.96      0.87      0.91        79\n",
            "\n",
            "    accuracy                           0.92      1242\n",
            "   macro avg       0.93      0.91      0.92      1242\n",
            "weighted avg       0.92      0.92      0.92      1242\n",
            "\n",
            "Number of components: 200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Almacenamiento de resultados.\n",
        "import joblib\n",
        "\n",
        "joblib.dump(results, 'results.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdJ82pZKkzFG",
        "outputId": "99f12429-f652-49d0-fd13-9d038b811145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['results.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nttneC2gZiHF"
      },
      "source": [
        "## 4. Responda las siguentes preguntas\n",
        "\n",
        "> **Pista:** ¿Como podrían explorar esta multiplicidad de opciones de forma automática?\n",
        "\n",
        "En particular deberan contestar las siguientes preguntas:\n",
        "\n",
        " - ¿Que métodos de reducción de dimensionalidad resultan mejores?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUvK--QbZyEw"
      },
      "source": [
        "```\n",
        "El método TruncatedSVD arrojó mejores resultados frente a Latent Dirichlet Allocation.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOY_rjcyZx3k"
      },
      "source": [
        " - ¿Que numero de componentes hace sentido para estre problema?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noL3w-Y2ZxnY"
      },
      "source": [
        "```\n",
        "De la ejecución del featurizer LDA, se obtuvo un número de componentes igual a 10. En cambio, para el\n",
        "featurizer Truncated SVD, el número de componentes ideal es 2.\n",
        "\n",
        "Sin embargo, se obtuvieron significativamente mejores resultados al utilizar un número mayor de componentes.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GOJdmagZxRE"
      },
      "source": [
        " - ¿Que tipo de modelos resultan mejores a la hora de ser utilizados como clasificadores? (estimator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjs4fdWdZ3Ae"
      },
      "source": [
        "```\n",
        "Ambas clases de estimadores arrojaron adecuados resultados cuando se utiliza un número de componentes lo\n",
        "significativamente grande para mejorar las predicciones del modelo global de clasificación de tópicos.\n",
        "\n",
        "El mejor pipeline obtenido comprendió el uso de TruncatedSVD como featurizer, con un número de componentes\n",
        "igual a 200, y tomando como clasificador el modelo de regresión logística. El accuracy obtenido es de 0,94.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LIV51kJfqyc"
      },
      "source": [
        "### Enviar trabajo práctico para evaluación\n",
        "\n",
        "Para enviar sus respuestas:\n",
        "\n",
        "1. Guarde una copia de este notebook en Google Colab.\n",
        "2. Comparta el notebook y copie la dirección URL al miso.\n",
        "\n",
        "  <img src='https://github.com/santiagxf/M72109/blob/master/docs/practice/_images/save_and_share.png?raw=1' width='700'/>\n",
        "\n",
        "3. Genere una entrega en el campus pretando atención a:\n",
        "\n",
        "  1. **Número de entrega** = 1.\n",
        "  2. **Comentario** = pegue el link que acaba de copiar.\n",
        "\n",
        "  <img src='https://github.com/santiagxf/M72109/blob/master/docs/practice/_images/share_and_submit.gif?raw=1' width='700'/>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1n_LptW1xRJ-",
        "fBLDj74FZWAq",
        "NCHelkwGxRKS"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
